
import os
import time
import logging
import warnings
from concurrent.futures import ThreadPoolExecutor, as_completed

from promptflow.client import PFClient, load_flow
from sqlalchemy import literal
from sqlalchemy.dialects.postgresql import JSONB

from common_svc.db.base import DBSession
from common_svc.logger.log_util import configure_loggers
from common_svc.storage.blob_client import setup_blob_resource
from common_svc.storage.blob_service import BlobService
from common_svc.config.config_store import BlobConfig

from fts_commons.utils.util import get_current_datetime, flatten_config, get_strategy_storage_path
from fts_commons.db.model import Task, Dataset, DataCurationStrategies, EvaluationStrategies
from fts_commons.utils.objects import TaskStatusEnum, DataCurationRuntimeConfig, EvaluationRuntimeConfig

warnings.filterwarnings("ignore")
configure_loggers()
logger = logging.getLogger(__name__)

PROMPTFLOW_ROOT_PATH = os.path.abspath("./promptflows")
os.makedirs(PROMPTFLOW_ROOT_PATH, exist_ok=True)

def fetch_and_lock_pending_task():
    try:
        with DBSession() as db:
            task = db.session.query(Task).filter(
                Task.status == literal('"Not Started"').cast(JSONB)
            ).order_by(Task.modified_ts.asc()).first()

            if task:
                logger.info(f"Locked and fetched task ID: {task.id}")
                task.status = TaskStatusEnum.IN_PROGRESS
                task.modified_ts = get_current_datetime()
                return task
            else:
                logger.info("No tasks found with status 'Not Started'")
                return None
    except Exception as e:
        logger.error(f"Error locking task: {e}", exc_info=True)
        raise

def merge_config(static_cfg, dynamic_cfg, runtime_cfg, is_eval=False):
    if is_eval:
        merged_dynamic = {
            "evalRunTimeParams": {
                **(dynamic_cfg.get("evalRunTimeParams") or {}),
                **(runtime_cfg.get("evalRunTimeParams") or {})
            }
        }
        return {
            "static": static_cfg,
            "dynamic": EvaluationRuntimeConfig(**merged_dynamic).dict()
        }
    else:
        merged_dynamic = {**(dynamic_cfg or {}), **(runtime_cfg or {})}
        return {
            "static": static_cfg,
            "dynamic": DataCurationRuntimeConfig(**merged_dynamic).dict()
        }

def process_promptflow_task(task):
    try:
        logger.info(f"Processing task: {task.id}")
        config = task.config or {}
        task_type = task.task_type.lower()

        with DBSession() as db:
            if task_type == "data_curation":
                dataset = db.session.query(Dataset).filter(Dataset.id == config['datasetId']).first()
                strategy = db.session.query(DataCurationStrategies).filter(
                    DataCurationStrategies.id == config['dataCurationStrategyId']
                ).first()
            elif task_type == "evaluation":
                dataset = db.session.query(Dataset).filter(Dataset.id == config['datasetId']).first()
                strategy = db.session.query(EvaluationStrategies).filter(
                    EvaluationStrategies.id == config['strategyId']
                ).first()
            else:
                raise Exception(f"Unsupported task_type: {task_type}")

        if not dataset or not strategy:
            raise Exception("Dataset or Strategy not found")

        static_cfg = strategy.config.get("static", {})
        dynamic_cfg = strategy.config.get("dynamic", {})
        runtime_cfg = config.get("data_curation_strategy_dynamic_config", {}) if task_type == "data_curation" \
                      else config.get("taskRuntimeConfig", {})

        # Fallback blob path
        if not runtime_cfg.get("blob_path") and task_type == "data_curation":
            runtime_cfg["blob_path"] = dataset.storage_path

        merged_config = merge_config(static_cfg, dynamic_cfg, runtime_cfg, is_eval=(task_type == "evaluation"))
        flat_inputs = flatten_config(merged_config)

        # Download strategy
        blob_service = BlobService()
        strategy_path = get_strategy_storage_path(strategy.id, strategy.name)
        blob_location = blob_service.blob_location(
            container_name=BlobConfig().BLOB_DOC_CONTAINER,
            absolute_path=strategy_path
        )
        local_path = os.path.join(PROMPTFLOW_ROOT_PATH, str(task.id))
        setup_blob_resource().copy_blob_dir_to_local(blob_location, local_path)

        local_path = os.path.join(local_path, "promptflow")
        pf = PFClient()
        flow = load_flow(local_path)
        result = flow.invoke(inputs=flat_inputs)

        if not result.output:
            raise Exception("Promptflow returned no output.")

        output_path_key = "dynamic_blob_path" if task_type == "data_curation" else "dynamic_evalRunTimeParams_final_blob_path"
        output_path = flat_inputs.get(output_path_key, "")

        with DBSession() as db:
            task_record = db.session.query(Task).filter(Task.id == task.id).first()
            if task_record:
                task_record.status = "Success"
                task_record.output = {
                    "outputDirectory": output_path,
                    "results": result.output
                }
                task_record.modified_ts = get_current_datetime()

    except Exception as e:
        logger.error(f"Promptflow failed for task {task.id}: {str(e)}", exc_info=True)
        with DBSession() as db:
            task_record = db.session.query(Task).filter(Task.id == task.id).one_or_none()
            if task_record:
                task_record.status = "Failure"
                task_record.output = {"error": str(e)}
                task_record.modified_ts = get_current_datetime()

def daemon_loop_parallel(max_workers=3):
    logger.info("Promptflow Parallel Daemon Started")
    while True:
        try:
            tasks = []
            for _ in range(max_workers):
                task = fetch_and_lock_pending_task()
                if task:
                    tasks.append(task)

            if tasks:
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    futures = {executor.submit(process_promptflow_task, task): task for task in tasks}
                    for future in as_completed(futures):
                        try:
                            future.result()
                        except Exception as e:
                            logger.error(f"Error processing task: {e}", exc_info=True)
            else:
                logger.info("No pending tasks. Sleeping for 60 seconds.")
                time.sleep(60)

        except Exception as e:
            logger.error(f"Daemon loop error: {e}", exc_info=True)
            time.sleep(60)

if __name__ == "__main__":
    daemon_loop_parallel(max_workers=3)
