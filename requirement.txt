import os
import pandas as pd
import configparser
from ast import literal_eval
from typing import Dict
from promptflow import tool
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge_score import rouge_scorer
from dotenv import load_dotenv

load_dotenv()

# Load credentials from config file
def load_ssrai_config(cfg_path="resource/ssrai_config.cfg") -> dict:
    config = configparser.ConfigParser()
    config.read(cfg_path)
    return {
        "host": config["DEFAULT"]["host"],
        "key": config["DEFAULT"]["team_consumer_key"],
        "secret": config["DEFAULT"]["keep_it_secret"],
        "auth_type": config["DEFAULT"].get("auth_type", "pat"),
        "api_version": config["DEFAULT"].get("api_version", "1.0"),
        "client_id": config["DEFAULT"]["rai_client_id"]
    }

# BLEU and ROUGE scoring
def compute_metrics(gndth_ans: str, llm_answer: str) -> Dict[str, float]:
    smoothie = SmoothingFunction().method4
    bleu_score = sentence_bleu([gndth_ans.split()], llm_answer.split(), smoothing_function=smoothie)
    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
    rouge_scores = scorer.score(gndth_ans, llm_answer)
    rougel_f1 = rouge_scores['rougeL'].fmeasure
    return {"BLEU": bleu_score, "ROUGE-L": rougel_f1}

# Question-type-wise scoring
def question_typewise_accur(df: pd.DataFrame) -> pd.DataFrame:
    fil_lst = ['Conceptual', 'Reasoning-Based', 'Inferential', 'Factual', 'Procedural']
    fil_dic = {}
    for fil in fil_lst:
        fil_df = df[df["Question Type"] == fil]
        fil_dic[fil + "_cnt"] = fil_df['business_comparison_score'].count()
        fil_dic[fil + "_mean"] = round(fil_df['business_comparison_score'].mean(), 2) if not fil_df.empty else 0.0
    df.reset_index(inplace=True)
    df["type_wise_overall_business"] = None
    df["type_wise_overall_business"] = df["type_wise_overall_business"].astype(object)
    df.at[0, "type_wise_overall_business"] = fil_dic
    return df

# Overall mean scoring
def get_mean(df_acc: pd.DataFrame, cols: list) -> pd.DataFrame:
    for col in cols:
        try:
            df_acc["overall_" + col] = None
            df_acc.loc[0, "overall_" + col] = round(df_acc[col].mean(), 2)
        except Exception as e:
            print(f"Failed to compute mean for {col}: {str(e)}")
    return df_acc

# Core LLM execution logic
def llm_as_judge(input_data: str, model_name: str, llm_as_judge_prompt: str) -> dict:
    try:
        cfg = load_ssrai_config()

        ssc_rai_client = SSRAIClient(
            host=cfg["host"],
            team_consumer_key=cfg["key"],
            keep_it_secret=cfg["secret"],
            auth_type=cfg["auth_type"],
            rai_client_id=cfg["client_id"],
            api_version=cfg["api_version"]
        )

        llm = ssc_rai_client.llm(wrapper="langchain", model_name=model_name)
        prompt = PromptTemplate.from_template(llm_as_judge_prompt)
        llm_chain = LLMChain(llm=llm, prompt=prompt)
        result = llm_chain.predict(input_data=input_data)
        return literal_eval(result)

    except Exception as e:
        return {"error": str(e)}

# Main Promptflow tool
@tool
def compute_scores_for_df(
    csv_path: str,
    output_path: str,
    model_name: str,
    llm_as_judge_prompt: str
) -> str:
    df = pd.read_csv(csv_path)
    formatted_output = []

    for index, row in df.iterrows():
        input_data = f"""# {index}
## question: {row['Question']}
## ground_truth_answer: {row['Answer']}
## generated_answer: {row['finetuned_response']}"""

        try:
            result = llm_as_judge(
                input_data=input_data,
                model_name=model_name,
                llm_as_judge_prompt=llm_as_judge_prompt
            )

            if result and isinstance(result, list):
                result_obj = result[0]
                formatted_output.append({
                    "business_comparison_score": round(
                        (result_obj['accuracy_score'] + result_obj['completeness_score']) / 2, 2),
                    **result_obj
                })
            else:
                formatted_output.append({
                    "business_comparison_score": None,
                    "accuracy_score": None,
                    "accuracy_explanation": "LLM response missing",
                    "completeness_score": None,
                    "completeness_explanation": "LLM response missing"
                })

        except Exception as e:
            formatted_output.append({
                "business_comparison_score": None,
                "accuracy_score": None,
                "accuracy_explanation": f"Error: {str(e)}",
                "completeness_score": None,
                "completeness_explanation": f"Error: {str(e)}"
            })

    df_scores = pd.DataFrame(formatted_output)
    df_final = pd.concat([df, df_scores], axis=1)
    df_final = question_typewise_accur(df_final)
    df_final = get_mean(df_final, ["business_comparison_score", "accuracy_score", "completeness_score"])
    df_final.to_excel(output_path, index=False)

    return output_path

