import os
import time
import logging
from promptflow.client import PFClient, load_flow
from common_svc.logger.log_util import configure_loggers
from common_svc.storage.blob_client import setup_blob_resource
from common_svc.storage.blob_service import BlobService
from common_svc.config.config_store import BlobConfig
from fts_commons.utils.util import get_current_datetime, flatten_config, get_strategy_storage_path
from fts_commons.db.model import Task, Dataset, DataCurationStrategies, EvaluationStrategy
from fts_commons.utils.objects import TaskStatusEnum, TaskTypeEnum
from sqlalchemy import literal
from common_svc.db.base import DBSession
import warnings

warnings.filterwarnings("ignore")
configure_loggers()
logger = logging.getLogger(__name__)

PROMPTFLOW_ROOT_PATH = os.path.abspath("./promptflows")
os.makedirs(PROMPTFLOW_ROOT_PATH, exist_ok=True)

def fetch_and_lock_pending_task():
    try:
        with DBSession() as db:
            task = db.session.query(Task)\
                .filter(Task.status == TaskStatusEnum.NOT_STARTED.value)\
                .order_by(Task.modified_ts.asc())\
                .first()
            if task:
                logger.info(f"Locked and fetched task ID: {task.id}")
                task.status = TaskStatusEnum.IN_PROGRESS.value
                task.modified_ts = get_current_datetime()
            return task
    except Exception as e:
        logger.error(f"Error locking task: {e}", exc_info=True)
        return None

def merge_dynamic_configs(static_cfg: dict, dynamic_cfg: dict, runtime_cfg: dict, task_type: str, dataset=None):
    if task_type == TaskTypeEnum.DATA_CURATION.value:
        if not runtime_cfg.get("blob_path") and dataset:
            runtime_cfg["blob_path"] = dataset.storage_path
    elif task_type == TaskTypeEnum.EVALUATION.value:
        if not runtime_cfg.get("evalRunTimeParams"):
            runtime_cfg["evalRunTimeParams"] = {}

    merged_dynamic = {**(dynamic_cfg or {}), **(runtime_cfg or {})}
    return {
        "static": static_cfg,
        "dynamic": merged_dynamic
    }

def process_promptflow_task(task):
    try:
        logger.info(f"Processing task: {task.id}")
        config = task.config or {}
        task_type = task.task_type

        with DBSession() as db:
            if task_type == TaskTypeEnum.DATA_CURATION.value:
                dataset = db.session.query(Dataset).filter(Dataset.id == config['datasetId']).first()
                strategy = db.session.query(DataCurationStrategies).filter(DataCurationStrategies.id == config['dataCurationStrategyId']).first()
            elif task_type == TaskTypeEnum.EVALUATION.value:
                dataset = db.session.query(Dataset).filter(Dataset.id == config['datasetId']).first()
                strategy = db.session.query(EvaluationStrategy).filter(EvaluationStrategy.id == config['strategyId']).first()
            else:
                raise ValueError(f"Unsupported task type: {task_type}")

        if not strategy or not dataset:
            raise Exception("Strategy or Dataset not found")

        strategy_path = get_strategy_storage_path(strategy.id, strategy.name)
        blob_service = BlobService()
        blob_location = blob_service.blob_location(
            container_name=BlobConfig().BLOB_DOC_CONTAINER,
            absolute_path=strategy_path
        )

        local_path = os.path.join(PROMPTFLOW_ROOT_PATH, str(task.id))
        os.makedirs(local_path, exist_ok=True)
        setup_blob_resource().copy_blob_dir_to_local(blob_location, local_path)

        strategy_cfg = strategy.config or {}
        static_cfg = strategy_cfg.get("static", {})
        dynamic_cfg = strategy_cfg.get("dynamic", {})
        runtime_cfg = config.get("data_curation_strategy_dynamic_config", {})

        merged_config = merge_dynamic_configs(static_cfg, dynamic_cfg, runtime_cfg, task_type, dataset)
        flat_inputs = flatten_config(merged_config)

        pf = PFClient()
        flow = load_flow(local_path)
        result = flow.invoke(inputs=flat_inputs)

        if not result.output:
            raise Exception("Promptflow returned no output.")

        output_key = "dynamic_blob_path" if task_type == TaskTypeEnum.DATA_CURATION.value else "dynamic_evalRunTimeParams_blob_path"
        output_path = flat_inputs.get(output_key, "")

        with DBSession() as db:
            task_record = db.session.query(Task).filter(Task.id == task.id).first()
            if task_record:
                task_record.status = TaskStatusEnum.SUCCESS.value
                task_record.output = {
                    "outputDirectory": output_path,
                    "results": result.output
                }
                task_record.modified_ts = get_current_datetime()

    except Exception as e:
        logger.error(f"Promptflow failed for task {task.id}: {str(e)}", exc_info=True)
        with DBSession() as db:
            task_record = db.session.query(Task).filter(Task.id == task.id).one_or_none()
            if task_record:
                task_record.status = TaskStatusEnum.FAILURE.value
                task_record.output = {
                    "error": str(e)
                }
                task_record.modified_ts = get_current_datetime()

def daemon_loop(max_workers=3):
    logger.info("Daemon started for Data Curation & Evaluation")
    while True:
        try:
            tasks = []
            for _ in range(max_workers):
                task = fetch_and_lock_pending_task()
                if task:
                    tasks.append(task)
            if tasks:
                for task in tasks:
                    process_promptflow_task(task)
            else:
                logger.info("No tasks pending. Sleeping for 60 seconds.")
                time.sleep(60)
        except Exception as e:
            logger.error(f"Daemon loop error: {e}", exc_info=True)
            time.sleep(60)
