
import os
import time
import logging
import warnings
from concurrent.futures import ThreadPoolExecutor, as_completed

from promptflow.client import PFClient, load_flow
from sqlalchemy import literal
from sqlalchemy.dialects.postgresql import JSONB

from common_svc.db.base import DBSession
from common_svc.logger.log_util import configure_loggers
from common_svc.storage.blob_client import setup_blob_resource
from common_svc.storage.blob_service import BlobService
from common_svc.config.config_store import BlobConfig

from fts_commons.utils.util import get_current_datetime, flatten_config, get_strategy_storage_path
from fts_commons.db.model import DataCurationStrategies, EvaluationStrategy, Dataset, Task
from fts_commons.utils.objects import TaskStatusEnum, TaskTypeEnum, DataCurationRuntimeConfig, EvalConfig, EvalStaticConfig, EvaluationRuntimeConfig

warnings.filterwarnings("ignore")

configure_loggers()
logger = logging.getLogger(__name__)

PROMPTFLOW_ROOT_PATH = os.path.abspath("./promptflows")
os.makedirs(PROMPTFLOW_ROOT_PATH, exist_ok=True)

def fetch_and_lock_pending_task():
    try:
        with DBSession() as db:
            task = (
                db.session.query(Task)
                .filter(Task.status == literal('"Not Started"').cast(JSONB))
                .order_by(Task.modified_ts.asc())
                .first()
            )
            if task:
                logger.info(f"Locked and fetched task ID: {task.id}")
                task.status = TaskStatusEnum.IN_PROGRESS
                task.modified_ts = get_current_datetime()
                return task
            else:
                logger.info("No tasks found with status 'Not Started'")
                return None
    except Exception as e:
        logger.error(f"Error locking task: {e}", exc_info=True)
        raise Exception("Failed to fetch and lock pending task.")

def merge_runtime_with_dynamic(static_cfg: dict, dynamic_cfg: dict, runtime_cfg: dict):
    merged_dynamic = {**(dynamic_cfg or {}), **(runtime_cfg or {})}
    return {
        "static": static_cfg,
        "dynamic": DataCurationRuntimeConfig(**merged_dynamic).dict()
    }

def process_promptflow_task(task):
    try:
        logger.info(f"Processing task: {task.id}")
        logger.info(f"TASK CONFIG: {task.config}")

        config = task.config or {}
        task_type = task.task_type

        if task_type == TaskTypeEnum.DATA_CURATION.value:
            if 'datasetId' not in config or 'dataCurationStrategyId' not in config:
                raise KeyError("Missing required keys in task.config. Required: 'datasetId', 'dataCurationStrategyId'")
        elif task_type == TaskTypeEnum.EVALUATION.value:
            if 'datasetId' not in config or 'strategyId' not in config:
                raise KeyError("Missing required keys in task.config. Required: 'datasetId', 'strategyId'")
        else:
            raise ValueError(f"Unsupported task type: {task_type}")

        with DBSession() as db:
            dataset = db.session.query(Dataset).filter(Dataset.id == config['datasetId']).first()

            if task_type == TaskTypeEnum.DATA_CURATION.value:
                strategy = db.session.query(DataCurationStrategies).filter(
                    DataCurationStrategies.id == config['dataCurationStrategyId']
                ).first()
            else:
                strategy = db.session.query(EvaluationStrategy).filter(
                    EvaluationStrategy.id == config['strategyId']
                ).first()

            if not dataset or not strategy:
                raise Exception("Dataset or strategy not found")

            blob_service = BlobService()
            blob_location = blob_service.blob_location(
                container_name=BlobConfig().BLOB_DOC_CONTAINER,
                absolute_path=get_strategy_storage_path(strategy.id, strategy.name)
            )

            local_path = os.path.join(PROMPTFLOW_ROOT_PATH, str(task.id))
            setup_blob_resource().copy_blob_dir_to_local(blob_location, local_path)

            static_cfg = strategy.config.get("static", {})
            dynamic_cfg = strategy.config.get("dynamic", {})
            runtime_cfg = config.get("data_curation_strategy_dynamic_config", {})

            if not runtime_cfg.get("blob_path"):
                runtime_cfg["blob_path"] = dataset.storage_path

            if task_type == TaskTypeEnum.DATA_CURATION.value:
                merged_config = merge_runtime_with_dynamic(static_cfg, dynamic_cfg, runtime_cfg)
            else:
                eval_config = EvalConfig(
                    static=EvalStaticConfig(**static_cfg),
                    dynamic=EvaluationRuntimeConfig(**dynamic_cfg)
                )
                merged_config = eval_config.dict()

            flat_inputs = flatten_config(merged_config)

            pf = PFClient()
            flow = load_flow(local_path)
            result = flow.invoke(inputs=flat_inputs)

            if not result.output:
                raise Exception("Promptflow returned no output.")

            logger.info(f"Flow result: {result.output}")

            output_path = result.output.get("final_blob_path") or flat_inputs.get("dynamic_blob_path", "")

            with DBSession() as db:
                task_record = db.session.query(Task).filter(Task.id == task.id).first()
                if task_record:
                    task_record.status = TaskStatusEnum.SUCCESS.value
                    task_record.output = {
                        "outputDirectory": output_path,
                        "results": result.output
                    }
                    task_record.modified_ts = get_current_datetime()

    except Exception as e:
        logger.error(f"Promptflow failed for task {task.id}: {str(e)}", exc_info=True)
        with DBSession() as db:
            task_record = db.session.query(Task).filter(Task.id == task.id).one_or_none()
            if task_record:
                task_record.status = TaskStatusEnum.FAILURE.value
                task_record.output = {
                    "error": str(e)
                }
                task_record.modified_ts = get_current_datetime()

def daemon_loop_parallel(max_workers=3):
    logger.info("Promptflow Parallel Daemon Started")
    while True:
        try:
            tasks = []
            for _ in range(max_workers):
                task = fetch_and_lock_pending_task()
                if task:
                    tasks.append(task)

            if tasks:
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    futures = {executor.submit(process_promptflow_task, task): task for task in tasks}
                    for future in as_completed(futures):
                        try:
                            future.result()
                        except Exception as e:
                            logger.error(f"Error processing task in parallel: {e}", exc_info=True)
            else:
                logger.info("No pending tasks. Sleeping for 60 seconds.")
                time.sleep(60)

        except Exception as e:
            logger.error(f"Daemon loop error: {e}", exc_info=True)
            time.sleep(60)
