
import io
import pandas as pd
import json
import logging
from ssrai_service import DataCurationService, BlobLocation

logger = logging.getLogger(__name__)

# Define a fallback class if StorageObj is not available from imports
class StorageObj:
    def __init__(self, code: int, message: str, data: dict = None):
        self.code = code
        self.message = message
        self.data = data or {}

    def __str__(self):
        return self.message

def save_evaluation_results_to_blob(df: pd.DataFrame, filename: str = "evaluation_results.csv") -> str:
    """
    Save a DataFrame to Azure Blob using existing upload_file_blob logic,
    and return the path string (not object) to avoid JSON serialization errors.
    """
    try:
        csv_buffer = io.StringIO()
        df.to_csv(csv_buffer, index=False)

        response = DataCurationService().upload_file_blob(
            csv_buffer.getvalue(),
            filename=filename,
            blobFilePath=BlobLocation(
                container_name="pvt-markets",
                absolute_path=filename
            )
        )

        # Handle different types of return formats safely
        if isinstance(response, StorageObj):
            logger.info(f"Upload successful. Message: {response.message}")
            return str(response)  # Ensures string
        elif isinstance(response, dict) and "storagePath" in response:
            return response["storagePath"]
        else:
            logger.warning("Unknown upload response type. Returning default filename.")
            return filename

    except Exception as e:
        logger.error(f"Error saving evaluation results to blob: {e}", exc_info=True)
        return ""
