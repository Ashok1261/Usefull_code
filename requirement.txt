
import os
import json
import logging
from promptflow.client import PFClient, load_flow
from common_svc.logger.log_util import configure_loggers
from common_svc.storage.blob_service import BlobService
from common_svc.config.config_store import BlobConfig
from common_svc.storage.blob_client import setup_blob_resource
from fts_commons.utils.util import generate_unique_number, get_strategy_storage_path, flatten_config
from fts_commons.utils.objects import (
    TaskStatusEnum, TriggerResponse, EvalConfig,
    EvalStaticConfig, EvaluationRuntimeConfig
)
from fts_commons.schema.user_model import UserModel
from fts_mgmt.utils.db_checks import validate_task
from fts_mgmt.dao.evaluation_strategy_mgmt_dao import EvaluationStrategyDAO
from fts_mgmt.dao.dataset_mgmt_dao import DataMgmtServiceDao
from fts_mgmt.dao.task_mgmt_dao import TaskMgmtDao

configure_loggers()
logger = logging.getLogger(__name__)

class TaskMgmtService:
    def __init__(self, userModel: UserModel):
        self.user_model = userModel
        self.task_mgmt_dao = TaskMgmtDao()
        self.data_mgmt_service_dao = DataMgmtServiceDao()
        self.es_service_daos_service_dao = EvaluationStrategyDAO()
        self.blob_service = BlobService()
        self.user_json = [{
            "id": self.user_model.userId,
            "name": f"{self.user_model.firstName} {self.user_model.lastName}"
        }]

    async def trigger_task(self, Task_type_enum, trigger_config):
        task_id = generate_unique_number()

        # Step 1: Validate runtime config
        trigger_config.validate_task_runtime_config(task_type=Task_type_enum)
        validate_task(trigger_config=trigger_config, task_type=Task_type_enum)

        strategy_id = trigger_config.strategyId
        dataset_id = trigger_config.datasetId
        project_id = trigger_config.projectId

        try:
            # Step 2: Fetch strategy config
            strategy_data = self.es_service_daos_service_dao.get_evaluation_strategy_by_id(strategy_id=strategy_id)
            config_wrapper = strategy_data.config

            if isinstance(config_wrapper, str):
                config_wrapper = json.loads(config_wrapper)

            config_raw = config_wrapper.get("config", config_wrapper)
            strategy_static = config_raw.get("static", {})
            strategy_dynamic = config_raw.get("dynamic", {})

            if not strategy_static or not strategy_dynamic:
                raise Exception("Missing 'static' or 'dynamic' in strategy config")

            if "evalRunTimeParams" not in strategy_dynamic:
                raise Exception("Missing 'evalRunTimeParams' in dynamic config")

            # Step 3: Merge runtime config with strategy, but skip overwriting with empty strings
            runtime_cfg = trigger_config.taskRuntimeConfig.dict(by_alias=True, exclude_none=True)

            merged_dynamic = {}
            for key, value in strategy_dynamic.items():
                if key in runtime_cfg and not (isinstance(runtime_cfg[key], str) and runtime_cfg[key].strip() == ""):
                    merged_dynamic[key] = runtime_cfg[key]
                else:
                    merged_dynamic[key] = value

            for key, value in runtime_cfg.items():
                if key not in merged_dynamic:
                    merged_dynamic[key] = value

            # Step 4: Build EvalConfig and flatten for Promptflow
            eval_config = EvalConfig(
                static=EvalStaticConfig(**strategy_static),
                dynamic=EvaluationRuntimeConfig(**merged_dynamic)
            )

            updated_config = eval_config.dict()
            flattened_config = flatten_config(updated_config)

            logger.info(f"ðŸ§¾ Flattened Promptflow config:\n{json.dumps(flattened_config, indent=2)}")

            # Step 5: Save final merged config to DB
            self.task_mgmt_dao.create_task(
                task_id=task_id,
                task_type=Task_type_enum,
                config=updated_config,  # âœ… Merged config stored
                status=TaskStatusEnum.IN_PROGRESS,
                created_by=self.user_json,
                project_id=project_id
            )

            # Step 6: Load Promptflow
            strategy_path = get_strategy_storage_path(strategy_id=strategy_id, strategy_name=strategy_data.name)
            absolute_path = self.blob_service.blob_location(
                container_name=BlobConfig().BLOB_DOC_CONTAINER,
                absolute_path=strategy_path
            )

            local_path = os.path.join(os.getcwd(), "promptflow")
            os.makedirs(local_path, exist_ok=True)
            setup_blob_resource().copy_blob_dir_to_local(absolute_path, os.path.dirname(local_path))

            pf_client = PFClient()
            pf_flow = load_flow(source=local_path)

            # Step 7: Run Promptflow
            flow_result = pf_flow.invoke(inputs=flattened_config)
            results = flow_result.output

            # Step 8: Update task as successful
            self.task_mgmt_dao.update_task(
                task_id=task_id,
                status=TaskStatusEnum.SUCCESS,
                output={"outputDirectory": flattened_config.get("dynamic_evalRunTimeParams_csv_path", "")}
            )

            task = self.task_mgmt_dao.get_task_by_id(task_id=task_id)

            return TriggerResponse(
                taskId=task.id,
                taskType=Task_type_enum,
                taskStatus=task.status,
                createdBy=task.created_by,
                createdTs=task.created_ts,
                modifiedBy=task.modified_by,
                modifiedTs=task.modified_ts
            )

        except Exception as e:
            error_msg = f"Evaluation task failed for Task ID {task_id}: {str(e)}"
            logger.error(error_msg, exc_info=True)
            self.task_mgmt_dao.update_task(
                task_id=task_id,
                status=TaskStatusEnum.FAILURE,
                output={"error": error_msg}
            )
            raise Exception(error_msg)
